---
title: "8.1 Assignment: Fit a Logistic Regression Model to the Thoracic Surgery Binary Dataset"
author: "Dan Siegel"
date: July 22, 2018
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(ggplot2)
library(dplyr) 
library(readr)
library(class)
library(caret)
library(gmodels)
library(cluster)
library(fpc)
library(foreign)
library(date)
library(ROCR)
surg <- read.arff('https://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff')
surg$Risk1Yr <- as.numeric(surg$Risk1Yr)-1
surg$DGN <- as.factor(surg$DGN)
surg$PRE6 <- as.factor(surg$PRE6)
surg$PRE7 <- as.numeric(surg$PRE7)-1
surg$PRE8 <- as.numeric(surg$PRE8)-1
surg$PRE9 <- as.numeric(surg$PRE9)-1
surg$PRE10 <- as.numeric(surg$PRE10)-1
surg$PRE11 <- as.numeric(surg$PRE11)-1
surg$PRE14 <- as.factor(surg$PRE14)
surg$PRE17 <- as.numeric(surg$PRE17)-1
surg$PRE19 <- as.numeric(surg$PRE19)-1
surg$PRE25 <- as.numeric(surg$PRE25)-1
surg$PRE30 <- as.numeric(surg$PRE30)-1
surg$PRE32 <- as.numeric(surg$PRE32)-1
```

# Assignment 8.2
## Fit the training data to a logistic regression model using the glm() function and use the test data to calculate the following classifier metrics.

```{r message=FALSE, echo=TRUE}
set.seed(123)
n<- nrow(surg)
shuffled <- surg[sample(n),]
train <- shuffled[1:round(0.7 * n),]
test <- shuffled[(round(0.7 * n) + 1):n,]
testing_survived <- glm(Risk1Yr ~ DGN + PRE4 + PRE5 + PRE6 + PRE7 + PRE8 +    
                          PRE9 + PRE10 + PRE11 + PRE14 + PRE17  + PRE19 + 
                          PRE25 + PRE30 + PRE32 + AGE, data=train)
predicted_data <- predict(testing_survived, newdata = test)

```

## a. Calculate precision, recall, and F1 score for your model using the test dataset.

```{r echo=TRUE, message=TRUE}
pred <- prediction(predicted_data, test$Risk1Yr)
RP.perf <- performance(pred, "prec", "rec")
f1_score <- performance(pred,"f")
ROC.perf <- performance(pred, "tpr", "fpr")
```

## b. Plot the receiver operating characteristic (ROC) curve using the test dataset. Additionally, calculate the area under the curve (AUC) value.
```{r echo=TRUE, message=TRUE}
plot (ROC.perf)
auc.tmp <- performance(pred,"auc");
auc <- as.numeric(auc.tmp@y.values)
auc
```

##c. Consider the case where you fit a logistic regression model. When you calculate the classifier metrics, you get an accuracy of 96%, but an AUC of 53%. Is this a good predictive model? Explain why or not.
The AUC is the measure of performance of a binary classifier. The performance is averaged across all possible decision thresholds. 
Wikipedia has a great summarization of the issue at hand:

"One recent explanation of the problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system, as well as ignoring the possibility of concavity repair, so that related alternative measures such as Informedness or DeltaP are recommended."