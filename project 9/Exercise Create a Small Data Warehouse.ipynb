{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gazetteer Data\n",
    "\n",
    "a. Create Unmanaged Tables\n",
    "\n",
    "The first step of this assignment involves loading the data from the CSV files, combining the file with the file for the other year, and saving it to disk as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warehouse_dir = 'dsc_650/warehouse'\n",
    "spark = SparkSession.builder.appName(\"DSC650Assignment5\").config(\"spark.sql.warehouse.dir\", warehouse_dir).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df1.unionAll(df2)\n",
    "df.write.saveAsTable('places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaz_2017_files = sc.wholeTextFiles('FileStore/tables/650/data/gazetteer/2017/*.csv').map(lambda x: x[0]).collect()\n",
    "gaz_2018_files = sc.wholeTextFiles('FileStore/tables/650/data/gazetteer/2018/*.csv').map(lambda x: x[0]).collect()\n",
    "gaz_zips = list(zip(gaz_2017_files, gaz_2018_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "re.findall('(\\w+).csv', gaz_zips[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(gaz_zips)):\n",
    "  table_name = re.findall('(\\w+).csv', gaz_zips[i][0])\n",
    "  first_table = gaz_zips[i][0]\n",
    "  second_table = gaz_zips[i][1]\n",
    "  df1 = spark.read.load(first_table,format='csv',sep=',',inferSchema=True,header=True)\n",
    "  df2 = spark.read.load(second_table,format='csv',sep=',',inferSchema=True,header=True)\n",
    "  df = df1.unionAll(df2)\n",
    "  df.write.saveAsTable(str(table_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">congressional_district\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "      880|\n",
       "+---------+\n",
       "\n",
       "core_based_statistical_areas\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "     1890|\n",
       "+---------+\n",
       "\n",
       "counties\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "     6440|\n",
       "+---------+\n",
       "\n",
       "county_subdivisions\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "    73261|\n",
       "+---------+\n",
       "\n",
       "elementary_schools\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "     3926|\n",
       "+---------+\n",
       "\n",
       "places\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "    59151|\n",
       "+---------+\n",
       "\n",
       "secondary_schools\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "      974|\n",
       "+---------+\n",
       "\n",
       "tracts\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "   148002|\n",
       "+---------+\n",
       "\n",
       "unified_school_districts\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "    21779|\n",
       "+---------+\n",
       "\n",
       "urban_areas\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "     7202|\n",
       "+---------+\n",
       "\n",
       "zip_code_tabulation_areas\n",
       "+---------+\n",
       "row_count|\n",
       "+---------+\n",
       "    66288|\n",
       "+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(gaz_zips)):\n",
    "  table_name = re.findall('(\\w+).csv', gaz_zips[i][0])[0]\n",
    "  query = \"SELECT COUNT(*) AS row_count FROM \" + table_name\n",
    "  print(table_name)\n",
    "  spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_external_table(table_name):\n",
    "  table_dir = os.path.join(warehouse_dir, table_name)\n",
    "  return spark.catalog.createExternalTable(table_name, table_dir)\n",
    "import os\n",
    "def create_external_tables():\n",
    "    for table_name in table_names:\n",
    "        create_external_table(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansigreen\">&lt;command-889483347589550&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n",
       "<span class=\"ansigreen\">      4</span>   table_name <span class=\"ansiyellow\">=</span> table_name <span class=\"ansiyellow\">+</span> <span class=\"ansiblue\">&apos;_external&apos;</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      5</span>   table_names<span class=\"ansiyellow\">.</span>append<span class=\"ansiyellow\">(</span>table_name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\">   </span>create_external_table<span class=\"ansiyellow\">(</span>table_name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      7</span> create_external_tables<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">&lt;command-889483347589549&gt;</span> in <span class=\"ansicyan\">create_external_table</span><span class=\"ansiblue\">(table_name)</span>\n",
       "<span class=\"ansigreen\">      2</span> <span class=\"ansigreen\">def</span> create_external_table<span class=\"ansiyellow\">(</span>table_name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      3</span>   table_dir <span class=\"ansiyellow\">=</span> os<span class=\"ansiyellow\">.</span>path<span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>warehouse_dir<span class=\"ansiyellow\">,</span> table_name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\">   </span><span class=\"ansigreen\">return</span> spark<span class=\"ansiyellow\">.</span>catalog<span class=\"ansiyellow\">.</span>createExternalTable<span class=\"ansiyellow\">(</span>table_name<span class=\"ansiyellow\">,</span> table_dir<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      5</span> <span class=\"ansigreen\">import</span> os<span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      6</span> <span class=\"ansigreen\">def</span> create_external_tables<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/catalog.py</span> in <span class=\"ansicyan\">createExternalTable</span><span class=\"ansiblue\">(self, tableName, path, source, schema, **options)</span>\n",
       "<span class=\"ansigreen\">    156</span>             <span class=\"ansiblue\">&quot;createExternalTable is deprecated since Spark 2.2, please use createTable instead.&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    157</span>             DeprecationWarning)\n",
       "<span class=\"ansigreen\">--&gt; 158</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>createTable<span class=\"ansiyellow\">(</span>tableName<span class=\"ansiyellow\">,</span> path<span class=\"ansiyellow\">,</span> source<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    159</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    160</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2.2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/catalog.py</span> in <span class=\"ansicyan\">createTable</span><span class=\"ansiblue\">(self, tableName, path, source, schema, **options)</span>\n",
       "<span class=\"ansigreen\">    180</span>                 &quot;spark.sql.sources.default&quot;, &quot;org.apache.spark.sql.parquet&quot;)\n",
       "<span class=\"ansigreen\">    181</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">--&gt; 182</span><span class=\"ansiyellow\">             </span>df <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jcatalog<span class=\"ansiyellow\">.</span>createTable<span class=\"ansiyellow\">(</span>tableName<span class=\"ansiyellow\">,</span> source<span class=\"ansiyellow\">,</span> options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    183</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    184</span>             <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> StructType<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n",
       "<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n",
       "<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n",
       "<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n",
       "<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansired\">AnalysisException</span>: &apos;Unable to infer schema for Parquet. It must be specified manually.;&apos;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_names = []\n",
    "for i in range(len(gaz_zips)):\n",
    "  table_name = re.findall('(\\w+).csv', gaz_zips[i][0])[0]\n",
    "  table_name = table_name + '_external'\n",
    "  table_names.append(table_name)\n",
    "  create_external_table(table_name)\n",
    "create_external_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Load and Query Tables\n",
    "\n",
    "Now that I have saved the data to external tables, I will load the tables back into Spark and create a report using Spark SQL. For this report, I will create a report on school districts for the states of Nebraska and Iowa using the elementary_schools, secondary_schools and unified_school_districts tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+----+-------+---------+----------+\n",
       "State|Year|Unified|Secondary|Elementary|\n",
       "+-----+----+-------+---------+----------+\n",
       "   IA|2017|    336|        0|         0|\n",
       "   IA|2018|    333|        0|         0|\n",
       "   NE|2018|    246|        0|         0|\n",
       "   NE|2017|    251|        0|         0|\n",
       "+-----+----+-------+---------+----------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Unified = spark.sql(\"SELECT state as State, year as Year, count(*) as Unified FROM unified_school_districts GROUP BY state, year\")\n",
    "Secondary = spark.sql(\"SELECT state as State, year as Year, count(*) as Secondary FROM secondary_schools GROUP BY state, year\")\n",
    "Elementary = spark.sql(\"SELECT state as State, year as Year, count(*) as Elementary FROM elementary_schools GROUP BY state, year\")\n",
    "s = Unified.join(Secondary,(Unified.Year == Secondary.Year) & (Unified.State == Secondary.State), 'left')\n",
    "z = s.join(Elementary,(Unified.Year == Elementary.Year) & (Unified.State == Elementary.State), 'left')\n",
    "z = z.drop(Secondary.Year).drop(Secondary.State).drop(Elementary.Year).drop(Elementary.State)\n",
    "z.na.fill(0).filter((z.State == 'NE') | (z.State == 'IA')).sort(z.State).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+----+-------+---------+----------+\n",
       "state|year|Unified|Secondary|Elementary|\n",
       "+-----+----+-------+---------+----------+\n",
       "   IA|2018|    333|        0|         0|\n",
       "   NE|2018|    246|        0|         0|\n",
       "   IA|2017|    336|        0|         0|\n",
       "   NE|2017|    251|        0|         0|\n",
       "+-----+----+-------+---------+----------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT unified_school_districts.state, unified_school_districts.year, count(unified_school_districts.state) as Unified, count(secondary_schools.state) \\\n",
    "        as Secondary, count(elementary_schools.state) as Elementary FROM unified_school_districts \\\n",
    "        LEFT JOIN secondary_schools on unified_school_districts.state = secondary_schools.state and unified_school_districts.year = secondary_schools.year \\\n",
    "        LEFT JOIN elementary_schools on unified_school_districts.state = elementary_schools.state and unified_school_districts.year = elementary_schools.year \\\n",
    "        WHERE unified_school_districts.state = 'IA' or unified_school_districts.state = 'NE' GROUP BY unified_school_districts.state, unified_school_districts.year \\\n",
    "        SORT BY unified_school_districts.state\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flight Data\n",
    "\n",
    "In the previous exercise, you joined data from flights and airport codes to create a report. Create an external table for airport_codes and domestic_flights from the domestic-flights/flights.parquet and airport-codes/airport-codes.csv files. From this I will create a report of the top ten airports for 2008 using Spark SQL instead of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_flights = spark.read.csv('/FileStore/tables/flights.csv',inferSchema=True,header=True)\n",
    "df_airport_codes = spark.read.csv('/FileStore/tables/airport_codes-e62ed.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">++\n",
       "|\n",
       "++\n",
       "++\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"CREATE VIEW flights_08 AS SELECT flights.origin_airport_code, airport_codes.name, flights.passengers, \\\n",
    "          flights.flight_date, flights.flights FROM flights \\\n",
    "          LEFT JOIN airport_codes ON flights.origin_airport_code = airport_codes.iata_code \\\n",
    "          WHERE flights.flight_date >= '2008-01-01' and flights.flight_date <= '2008-12-31' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------------+--------------------+----------+-------------------+-------+\n",
       "origin_airport_code|                name|passengers|        flight_date|flights|\n",
       "+-------------------+--------------------+----------+-------------------+-------+\n",
       "                MHK|Manhattan Regiona...|        21|2008-10-01 00:00:00|      1|\n",
       "                SEA|Seattle Tacoma In...|       126|2008-11-01 00:00:00|      4|\n",
       "                SEA|Seattle Tacoma In...|        61|2008-11-01 00:00:00|      1|\n",
       "                SEA|Seattle Tacoma In...|       666|2008-02-01 00:00:00|     29|\n",
       "                GEG|Spokane Internati...|        13|2008-02-01 00:00:00|      1|\n",
       "                SEA|Seattle Tacoma In...|         0|2008-08-01 00:00:00|      1|\n",
       "                SEA|Seattle Tacoma In...|      4491|2008-04-01 00:00:00|    113|\n",
       "                SEA|Seattle Tacoma In...|        37|2008-04-01 00:00:00|      1|\n",
       "                AZA|Phoenix-Mesa-Gate...|       883|2008-11-01 00:00:00|      8|\n",
       "                SEA|Seattle Tacoma In...|      3858|2008-09-01 00:00:00|     82|\n",
       "                SEA|Seattle Tacoma In...|      5045|2008-05-01 00:00:00|    115|\n",
       "                SEA|Seattle Tacoma In...|        59|2008-05-01 00:00:00|      1|\n",
       "                AZA|Phoenix-Mesa-Gate...|       274|2008-10-01 00:00:00|      3|\n",
       "                SEA|Seattle Tacoma In...|      4598|2008-02-01 00:00:00|     80|\n",
       "                SEA|Seattle Tacoma In...|       639|2008-01-01 00:00:00|     31|\n",
       "                SEA|Seattle Tacoma In...|        68|2008-09-01 00:00:00|      2|\n",
       "                BIL|Billings Logan In...|       100|2008-08-01 00:00:00|      1|\n",
       "                SEA|Seattle Tacoma In...|      4164|2008-01-01 00:00:00|     86|\n",
       "                AZA|Phoenix-Mesa-Gate...|      1281|2008-12-01 00:00:00|     10|\n",
       "                SEA|Seattle Tacoma In...|      4542|2008-12-01 00:00:00|     82|\n",
       "+-------------------+--------------------+----------+-------------------+-------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM flights_08 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+---------+---------------------+------------------------+------------------------+\n",
       "                Name|IATA code|Total Inbound Flights|Total Inbound Passengers|Average Daily Passengers|\n",
       "+--------------------+---------+---------------------+------------------------+------------------------+\n",
       "Hartsfield Jackso...|      ATL|               395729|                35435896|       4097.109029945658|\n",
       "Chicago O&apos;Hare In...|      ORD|               357181|                26422032|      2799.5371900826444|\n",
       "Dallas Fort Worth...|      DFW|               270055|                22835496|       4659.354417465824|\n",
       "Los Angeles Inter...|      LAX|               215359|                19757561|       4029.688150112176|\n",
       "George Bush Inter...|      IAH|               213650|                14854413|       2680.334355828221|\n",
       "Charlotte Douglas...|      CLT|               205650|                15009641|       2420.519432349621|\n",
       "Detroit Metropoli...|      DTW|               192531|                14187841|      2482.1275367389785|\n",
       "         Erase Me 19|      PHL|               188753|                12818872|      2193.1346449957227|\n",
       "Philadelphia Inte...|      PHL|               188753|                12818872|      2193.1346449957227|\n",
       "Phoenix Sky Harbo...|      PHX|               181608|                17237993|      3827.2630994671404|\n",
       "+--------------------+---------+---------------------+------------------------+------------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT name AS Name, origin_airport_code as `IATA code`, SUM(flights) AS `Total Inbound Flights`, SUM(passengers) as `Total Inbound Passengers`, MEAN(passengers) as `Average Daily Passengers` FROM flights_08 p1\\\n",
    "  GROUP BY origin_airport_code, name ORDER BY `Total Inbound Flights` DESC LIMIT 10\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "5.2 Programming Exercise  Create a Small Data Warehouse",
  "notebookId": 889483347589536
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
